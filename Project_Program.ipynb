{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BachNguyenT/Project-Program-W23/blob/main/Project_Program.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Establishing a Reddit Instance with Praw**"
      ],
      "metadata": {
        "id": "f1k44W4x9deH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install praw\n",
        "!pip install emoji\n",
        "!pip install re\n"
      ],
      "metadata": {
        "id": "Ne-HKZ_R8iDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMzFImlR7g4X"
      },
      "outputs": [],
      "source": [
        "#For gathering data from Reddit\n",
        "import praw #Python Reddit API wrapper\n",
        "import pandas as pd #DataFrame etc.\n",
        "from praw.models import MoreComments\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk import FreqDist\n",
        "import emoji #Remove emojis\n",
        "import re #Remove links\n",
        "import en_core_web_sm\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a reddit connection with reddit API details\n",
        "reddit = praw.Reddit(client_id='fpnqOLAN-SWfPgUnpLkfNg',\n",
        "                     client_secret='yhu-5vXFUVbS54RWBVTPcUA7_6jTRw',\n",
        "                     user_agent='Project Program UW')"
      ],
      "metadata": {
        "id": "pvJBEtZR83ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Obtaining Comments From A Post**"
      ],
      "metadata": {
        "id": "XfzrhxTP9m8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit = reddit.subreddit('uwaterloo')\n",
        "for submission in subreddit.hot(limit = 100):\n",
        "  print(submission.title) #Output: the submissions title\n",
        "  print('Submission ID =', submission.id, '\\n') #Output: the submission's id"
      ],
      "metadata": {
        "id": "kPse-8M49tCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining a Submission Object**"
      ],
      "metadata": {
        "id": "oE1N9WtP_3PO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Post1 = reddit.submission(id='103fj53' )"
      ],
      "metadata": {
        "id": "CINWWkSS-Y1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storing all Comments Scraped in a List**"
      ],
      "metadata": {
        "id": "w8u_JnGs-68E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Comments_All = []\n",
        "Post1.comments.replace_more(limit = 100)\n",
        "for comments in Post1.comments.list():\n",
        "  Comments_All.append(comments.body)\n",
        "\n",
        "print(Comments_All, '\\n')\n",
        "print('Total Comments Scraped = ', len(Comments_All))"
      ],
      "metadata": {
        "id": "m4Ut_c3pAAvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Preprocess The Comments**\n",
        "- removing emojis\n",
        "- tokenizing, removing links, etc\n",
        "- removing stopwards\n",
        "- normalizing words via lemmatizing\n"
      ],
      "metadata": {
        "id": "okasij7nAdRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing String - Convert To A String Object**"
      ],
      "metadata": {
        "id": "2ZyWrGo-AwxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "List1 = Comments_All\n",
        "List1 = [str(i) for i in List1] #Map to a list of strings\n",
        "string_uncleaned = ' , '.join(List1) #join all the strings separated by a comma\n",
        "string_uncleaned"
      ],
      "metadata": {
        "id": "vL-h7tmDBHXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing emojis"
      ],
      "metadata": {
        "id": "UNFmp9X4BWf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_emojiless = emoji.demojize(string_uncleaned)\n",
        "#If want to remove emoji codes too\n",
        "#example: ðŸ˜Š becomes :smiling_face_with_smiling_eyes:\n",
        "#string_emojiless = re.sub(r':[a-z_]+:', '', string_emojiless)"
      ],
      "metadata": {
        "id": "IOpnsn24BcC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing breaks apart every word in the string\n",
        "#Into an individual word\n",
        "#Which would then carry its own 'pos' or 'neg' sentiment\n",
        "#Based on our sentiment analyzer later"
      ],
      "metadata": {
        "id": "bVnBLU_eFYLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing & Cleaning Strings"
      ],
      "metadata": {
        "id": "jpqYUe1nFpEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|http\\S+')\n",
        "tokenized_string = tokenizer.tokenize(string_emojiless)\n",
        "print(tokenized_string)"
      ],
      "metadata": {
        "id": "CjGck_BMFuLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting Tokens Into Lowercase"
      ],
      "metadata": {
        "id": "skYjOA_eGQNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_string_tokenized = [word.lower() for word in tokenized_string]\n",
        "print(lower_string_tokenized)"
      ],
      "metadata": {
        "id": "NEJcLen3GUFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords"
      ],
      "metadata": {
        "id": "x5Qd5xdLGbNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "all_stopwords = nlp.Defaults.stop_words\n",
        "\n",
        "text = lower_string_tokenized\n",
        "tokens_without_sw = [word for word in text if not word in all_stopwords]\n",
        "\n",
        "print(tokens_without_sw)"
      ],
      "metadata": {
        "id": "9WCDEh1vGkaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing Words via Lemmatizing"
      ],
      "metadata": {
        "id": "w8FrZbr9Gxmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_tokens = ([lemmatizer.lemmatize(w) for w in tokens_without_sw])\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "3_t4thEnHRva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing Words via Stemming"
      ],
      "metadata": {
        "id": "X2n03beAI7j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "stem_tokens = ([stemmer.stem(s) for s in tokens_without_sw])\n",
        "print(stem_tokens)"
      ],
      "metadata": {
        "id": "aK8gRsxcJDtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_output = lemmatized_tokens"
      ],
      "metadata": {
        "id": "viUKGdmBJVVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original length of words = \", (len(string_uncleaned)))\n",
        "print(\"Number of words after removing emojis = \", (len(string_emojiless)))\n",
        "print(\"Number of words after removing tokenizing and cleaning = \", (len(tokenized_string)))\n",
        "print(\"Number of words after removing tokenizing, cleaning and removing stop words = \", (len(tokens_without_sw)))\n",
        "print(\"Number of words after removing tokenizing, cleaning, removing stop words and lemmatized = \", (len(lemmatized_tokens)))\n",
        "print(\"Number of words for final output = \", (len(cleaned_output)))"
      ],
      "metadata": {
        "id": "pNlqOYv1JYaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Apply A Sentiment Analyzer (VADER)**"
      ],
      "metadata": {
        "id": "KGYDNTrXJ7bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Calculate each tokenized word's polarity scores using the VADER \n",
        "(Valence Aware Dictionary for Sentiment Reasoning) model\n",
        "The polarity scores measure the positivity and negativity for each word.\n",
        "Compound score:normalized to be between -1 (neg) and +1 (pos). \n",
        "This provides a single unidimensional measure of sentiment for a given word.\n",
        "\n",
        "We would then store this as a data frame object.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "PxEs--FkKAAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polarity Score of Words"
      ],
      "metadata": {
        "id": "gek5dQkjKVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
        "\n",
        "sia = SIA()\n",
        "\n",
        "#Define custom polarity scores\n",
        "custom_polarity = {\n",
        "    'lol' : -0.2\n",
        "}\n",
        "\n",
        "sia.lexicon.update(custom_polarity)\n",
        "\n",
        "results = []\n",
        "\n",
        "for sentences in cleaned_output:\n",
        "  pol_score = sia.polarity_scores(sentences)\n",
        "  pol_score['words'] = sentences\n",
        "  results.append(pol_score)\n",
        "\n",
        "pd.set_option('display.max_columns', None, 'max_colwidth', None)\n",
        "df = pd.DataFrame.from_records(results)\n",
        "df"
      ],
      "metadata": {
        "id": "ZkdIOr4AKYHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter words based on the compound score and our criteria\n",
        "#We use a compound score of +- 0.10\n",
        "#Threshold values:\n",
        "\"\"\"\n",
        ">=0.05 = pos\n",
        "<= -0.05 = neg\n",
        "0.05 ~ -0.05 = neutral\n",
        "\"\"\"\n",
        "\n",
        "df['label'] = 0\n",
        "df.loc[df['compound'] > 0.10, 'label'] = 1\n",
        "df.loc[df['compound'] < 0, 'label'] = -1\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8-T7af1wMJvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Representation of Sentiment Results**"
      ],
      "metadata": {
        "id": "tOrkBkqcM0td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.label.value_counts())"
      ],
      "metadata": {
        "id": "WvzXFE9wM5uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representation Of Sentiments of Words"
      ],
      "metadata": {
        "id": "DocvO5QUOXGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fix, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "counts = df.label.value_counts(normalize = True) * 100\n",
        "\n",
        "sns.barplot(x=counts.index, y = counts, ax = ax)\n",
        "\n",
        "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
        "ax.set_ylabel(\"Percentage\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vI_BP086OazG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_positive_negative = df.loc[df['label'] != 0]\n",
        "df_positive_negative.head()"
      ],
      "metadata": {
        "id": "wMYtcicAPEYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_positive_negative.label.value_counts())"
      ],
      "metadata": {
        "id": "LpTyIpriPXGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (8,8))\n",
        "counts = df_positive_negative.label.value_counts(normalize = True)*100\n",
        "\n",
        "sns.barplot(x=counts.index, y=counts, ax=ax)\n",
        "\n",
        "ax.set_xticklabels(['Negative', 'Positive'])\n",
        "ax.set_ylabel(\"Percentage\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j_wqvvXvPfH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Visulizations**"
      ],
      "metadata": {
        "id": "SKxG_HvhQFZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution of Most Common Positive Words"
      ],
      "metadata": {
        "id": "1H-uFPzMPzqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = list(df.loc[df['label'] == 1].words)\n",
        "print(positive_words)"
      ],
      "metadata": {
        "id": "H9rKLBNCQQva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_frequency = FreqDist(positive_words)\n",
        "pos_freq = positive_frequency.most_common(30)\n",
        "pos_freq"
      ],
      "metadata": {
        "id": "_6D4NOYWQZJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequency Distribution of Most Common Negative Words"
      ],
      "metadata": {
        "id": "A5WHzqQXQ1Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = list(df.loc[df['label'] == -1].words)\n",
        "print(negative_words)"
      ],
      "metadata": {
        "id": "76ux41jVQ6xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_frequency = FreqDist(negative_words)\n",
        "neg_freq = negative_frequency.most_common(30)\n",
        "neg_freq"
      ],
      "metadata": {
        "id": "U64MWiI-RJSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization Via WordCloud**"
      ],
      "metadata": {
        "id": "6iYZuoxbUEDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Pos_words = [str(p) for p in pos_freq]\n",
        "Pos_words_string = ' , '.join(Pos_words)"
      ],
      "metadata": {
        "id": "B-w4KEo6UHTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Neg_words = [str(n) for n in neg_freq]\n",
        "Neg_words_string = ' , '.join(Neg_words)"
      ],
      "metadata": {
        "id": "1XdNjCVoUSq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Create and generate a word cloud image\n",
        "wordcloud_positive = WordCloud(background_color = \"white\").generate(Pos_words_string)\n",
        "wordcloud_negative = WordCloud().generate(Neg_words_string)\n",
        "\n",
        "#Display the generated image for Positive words\n",
        "plt.imshow(wordcloud_positive, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "#Display the generated image for Negative words\n",
        "plt.imshow(wordcloud_negative, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eWbyvf8hUfMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bar Chart Of Most Common Positive and Negative Words By Count**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kEebYAGZVTfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "pos_freq_df = pd.DataFrame(pos_freq)\n",
        "pos_freq_df = pos_freq_df.rename(columns = {0: 'Bar Graph Of Frequent Words', 1: 'Count'}, inplace = False)\n",
        "\n",
        "fig = px.bar(pos_freq_df, x = 'Bar Graph Of Frequent Words', y = 'Count', title = 'Commonly Used Positive Words By Count')\n",
        "fig.show()\n",
        "\n",
        "neg_freq_df = pd.DataFrame(neg_freq)\n",
        "neg_freq_df = neg_freq_df.rename(columns = {0: 'Bar Graph Of Frequent Words', 1: 'Count'}, inplace = False)\n",
        "\n",
        "fig = px.bar(neg_freq_df, x = 'Bar Graph Of Frequent Words', y = 'Count', title = 'Commonly Used Negative Words By Count')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "qizRXxjVVZ38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}